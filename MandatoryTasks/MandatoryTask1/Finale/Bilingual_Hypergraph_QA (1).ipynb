{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koNovyyzU6ZW"
      },
      "source": [
        "# Bilingual Hypergraph QA — Colab cell-by-cell\n"
      ],
      "id": "koNovyyzU6ZW"
    },
    {
      "cell_type": "code",
      "source": [
        "#Installin"
      ],
      "metadata": {
        "id": "ZtWSFmlCW1d1"
      },
      "id": "ZtWSFmlCW1d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "!pip install -q transformers datasets sentencepiece accelerate"
      ],
      "metadata": {
        "id": "nXntpimOV_cT"
      },
      "id": "nXntpimOV_cT",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "wWVobdZ0XAiO"
      },
      "id": "wWVobdZ0XAiO"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)"
      ],
      "metadata": {
        "id": "MWsPs39jWDeq"
      },
      "id": "MWsPs39jWDeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ultilits"
      ],
      "metadata": {
        "id": "sBXLDLuzXCvj"
      },
      "id": "sBXLDLuzXCvj"
    },
    {
      "cell_type": "code",
      "source": [
        "## 3) Utilities: set_seed, normalize, EM/F1 (simple)\n",
        "\n",
        "# %%\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "# simple string normalize for EM/F1 (not exhaustive)\n",
        "import re\n",
        "\n",
        "def normalize_answer(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# simple f1 and exact match for demonstration\n",
        "\n",
        "def compute_em_f1(pred: str, gold: str) -> Tuple[int, float]:\n",
        "    pred_n = normalize_answer(pred)\n",
        "    gold_n = normalize_answer(gold)\n",
        "    em = int(pred_n == gold_n)\n",
        "    pred_tokens = pred_n.split()\n",
        "    gold_tokens = gold_n.split()\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        f1 = 1.0 if pred_tokens == gold_tokens else 0.0\n",
        "    else:\n",
        "        common = set(pred_tokens) & set(gold_tokens)\n",
        "        num_same = sum(min(pred_tokens.count(w), gold_tokens.count(w)) for w in common)\n",
        "        if num_same == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            precision = num_same / len(pred_tokens)\n",
        "            recall = num_same / len(gold_tokens)\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "    return em, f1"
      ],
      "metadata": {
        "id": "OdpDANNtWIS5"
      },
      "id": "OdpDANNtWIS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "3dHsR_spXJHr"
      },
      "id": "3dHsR_spXJHr"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'xlm-roberta-base'  # multilingual encoder\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "base_model.eval()  # we'll use it as an encoder; optionally fine-tune\n",
        "\n",
        "print('Tokenizer vocab size:', tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "EtOfZntBWNIA"
      },
      "id": "EtOfZntBWNIA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny synthetic dataset"
      ],
      "metadata": {
        "id": "50nSsF4vXK3n"
      },
      "id": "50nSsF4vXK3n"
    },
    {
      "cell_type": "code",
      "source": [
        "## 5) Example data format and a tiny synthetic dataset\n",
        "\n",
        "# We'll create a tiny synthetic example showing English + French contexts and a span answer.\n",
        "# In practice you'd load a bilingual dataset (e.g., translated SQuAD) where each example has English context, French context (aligned), question, and answer span(s).\n",
        "\n",
        "# %%\n",
        "examples = [\n",
        "    {\n",
        "        'id': 'example-1',\n",
        "        'question_en': 'What color is the cat?',\n",
        "        'context_en': 'The cat sat on the mat. The cat is black and white.',\n",
        "        'context_fr': \"Le chat est assis sur le tapis. Le chat est noir et blanc.\",\n",
        "        'answer_text': 'black and white',\n",
        "        'answer_start': 34  # index in the English context for demonstration\n",
        "    }\n",
        "]\n",
        "\n",
        "# A helper to find token-level start/end after tokenization (approximate for the demo)\n",
        "\n",
        "def encode_example(example, tokenizer, max_length=384):\n",
        "    q = example['question_en']\n",
        "    c_en = example['context_en']\n",
        "    c_fr = example['context_fr']\n",
        "\n",
        "    # Tokenize contexts separately (we will keep token-level mapping separate)\n",
        "    enc_q = tokenizer(q, add_special_tokens=False)\n",
        "    enc_en = tokenizer(c_en, add_special_tokens=False)\n",
        "    enc_fr = tokenizer(c_fr, add_special_tokens=False)\n",
        "\n",
        "    return {\n",
        "        'id': example['id'],\n",
        "        'question_tokens': enc_q,\n",
        "        'en_tokens': enc_en,\n",
        "        'fr_tokens': enc_fr,\n",
        "        'answer_text': example['answer_text']\n",
        "    }\n",
        "\n",
        "encoded = [encode_example(e, tokenizer) for e in examples]"
      ],
      "metadata": {
        "id": "A-Vr87RFWQsn"
      },
      "id": "A-Vr87RFWQsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyper graph construction"
      ],
      "metadata": {
        "id": "o1YDV72fXOwP"
      },
      "id": "o1YDV72fXOwP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We'll construct simple hyperedges:\n",
        "# - sentence-level hyperedges (all tokens in a sentence)\n",
        "# - question-centered hyperedge (all tokens in the question)\n",
        "# - sliding-window phrase hyperedges (n-grams)\n",
        "\n",
        "# We'll represent hypergraph by incidence matrix H (num_nodes x num_hyperedges)\n",
        "\n",
        "# %%\n",
        "\n",
        "def build_simple_hypergraph(num_nodes: int, sentence_boundaries: List[Tuple[int,int]],\n",
        "                            question_node_indices: List[int], window_size=4) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Return H (num_nodes x num_hyperedges) as float tensor.\n",
        "    sentence_boundaries: list of (start_idx, end_idx) inclusive indices for sentences\n",
        "    question_node_indices: list of token indices that belong to the question (if any)\n",
        "    \"\"\"\n",
        "    hyperedges = []\n",
        "    # sentence hyperedges\n",
        "    for (s,e) in sentence_boundaries:\n",
        "        members = list(range(s, e+1))\n",
        "        hyperedges.append(members)\n",
        "    # question hyperedge\n",
        "    if question_node_indices:\n",
        "        hyperedges.append(list(question_node_indices))\n",
        "    # sliding windows over tokens\n",
        "    for i in range(0, num_nodes, max(1, window_size//2)):\n",
        "        members = list(range(i, min(i+window_size, num_nodes)))\n",
        "        if len(members) > 0:\n",
        "            hyperedges.append(members)\n",
        "\n",
        "    H = torch.zeros((num_nodes, len(hyperedges)), dtype=torch.float)\n",
        "    for j, members in enumerate(hyperedges):\n",
        "        for idx in members:\n",
        "            if 0 <= idx < num_nodes:\n",
        "                H[idx, j] = 1.0\n",
        "    return H\n",
        "\n",
        "# normalize incidence for hypergraph convolution\n",
        "\n",
        "def hypergraph_normalization(H: torch.Tensor):\n",
        "    # H: N x E\n",
        "    N, E = H.shape\n",
        "    deg_v = torch.clamp(H.sum(dim=1), min=1.0)  # node degrees\n",
        "    deg_e = torch.clamp(H.sum(dim=0), min=1.0)  # hyperedge degrees\n",
        "    Dv_inv_sqrt = torch.diag(torch.pow(deg_v, -0.5))\n",
        "    De_inv = torch.diag(1.0 / deg_e)\n",
        "    return Dv_inv_sqrt, De_inv"
      ],
      "metadata": {
        "id": "sBGPe3FsWT7e"
      },
      "id": "sBGPe3FsWT7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypergraph Conv layer"
      ],
      "metadata": {
        "id": "EicyjZz0XTvL"
      },
      "id": "EicyjZz0XTvL"
    },
    {
      "cell_type": "code",
      "source": [
        "## 7) Hypergraph convolution layer (PyTorch)\n",
        "\n",
        "# Equation (one commonly used form):\n",
        "# X' = D_v^{-1/2} H W_e^{-1} H^T D_v^{-1/2} X W\n",
        "# where H is incidence matrix (N x E), W_e is diagonal of hyperedge sizes (E x E) (we used De_inv above),\n",
        "# and W is learnable weight matrix.\n",
        "\n",
        "# %%\n",
        "\n",
        "class HyperGraphConv(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=True):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "    def forward(self, X: torch.Tensor, H: torch.Tensor):\n",
        "        # X: N x F, H: N x E\n",
        "        # compute normalization\n",
        "        Dv_inv_sqrt, De_inv = hypergraph_normalization(H)\n",
        "        # propagation\n",
        "        # step 1: H^T Dv^{-1/2} X\n",
        "        X_ = Dv_inv_sqrt @ X  # N x F\n",
        "        tmp = H.t() @ X_      # E x F\n",
        "        tmp = De_inv @ tmp    # E x F\n",
        "        out = H @ tmp         # N x F\n",
        "        out = Dv_inv_sqrt @ out  # N x F\n",
        "        out = self.linear(out)   # N x out_dim\n",
        "        return out\n",
        "\n",
        "# simple block\n",
        "class HyperGCNBlock(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        hidden_dim = hidden_dim or dim\n",
        "        self.conv = HyperGraphConv(dim, hidden_dim)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, X, H):\n",
        "        h = self.conv(X, H)\n",
        "        h = self.act(self.norm(h))\n",
        "        return self.dropout(h)"
      ],
      "metadata": {
        "id": "8U2l01wmWX0o"
      },
      "id": "8U2l01wmWX0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross attention model"
      ],
      "metadata": {
        "id": "I5P8x6S8XW54"
      },
      "id": "I5P8x6S8XW54"
    },
    {
      "cell_type": "code",
      "source": [
        "## 8) Cross-attention module (English<->French)\n",
        "\n",
        "# We'll implement a simple cross-attention using PyTorch's MultiheadAttention.\n",
        "# English tokens attend to French representations and vice versa. Outputs are residual-style fused representations.\n",
        "\n",
        "# %%\n",
        "\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, dim, n_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.mha_en_to_fr = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
        "        self.mha_fr_to_en = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm_en = nn.LayerNorm(dim)\n",
        "        self.norm_fr = nn.LayerNorm(dim)\n",
        "        self.ff = nn.Sequential(nn.Linear(2*dim, dim), nn.GELU(), nn.Linear(dim, dim))\n",
        "\n",
        "    def forward(self, en_repr, fr_repr, en_mask=None, fr_mask=None):\n",
        "        # en_repr: B x N_en x D, fr_repr: B x N_fr x D\n",
        "        # use mha: query=EN, key/value=FR -> EN attends to FR\n",
        "        en_attended, _ = self.mha_en_to_fr(query=en_repr, key=fr_repr, value=fr_repr, key_padding_mask=fr_mask)\n",
        "        fr_attended, _ = self.mha_fr_to_en(query=fr_repr, key=en_repr, value=en_repr, key_padding_mask=en_mask)\n",
        "\n",
        "        # residual + norm\n",
        "        en_fused = self.norm_en(en_repr + en_attended)\n",
        "        fr_fused = self.norm_fr(fr_repr + fr_attended)\n",
        "\n",
        "        # cross-fusion: concatenate corresponding pooled vectors (simple)\n",
        "        # We will broadcast-average each sequence and then fuse\n",
        "        en_pool = en_fused.mean(dim=1)  # B x D\n",
        "        fr_pool = fr_fused.mean(dim=1)\n",
        "        combined = torch.cat([en_pool, fr_pool], dim=-1)  # B x 2D\n",
        "        fused = self.ff(combined)  # B x D\n",
        "        return en_fused, fr_fused, fused"
      ],
      "metadata": {
        "id": "eTVa8LGQWa-0"
      },
      "id": "eTVa8LGQWa-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model assembly"
      ],
      "metadata": {
        "id": "oloRrON5XaoS"
      },
      "id": "oloRrON5XaoS"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Components:\n",
        "# - Transformer encoder (we'll reuse `base_model` to get contextual token embeddings for each language separately).\n",
        "# - HyperGCN blocks on each stream.\n",
        "# - Cross-attention fusion.\n",
        "# - Prediction head for start/end logits over English tokens (or both languages; for now predict on English tokens but use French info).\n",
        "\n",
        "# %%\n",
        "\n",
        "class BilingualHypergraphQA(nn.Module):\n",
        "    def __init__(self, encoder, hidden_dim=768, hyper_layers=2, attn_heads=8, dropout=0.1, freeze_encoder=False):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.hidden_dim = hidden_dim\n",
        "        if freeze_encoder:\n",
        "            for p in self.encoder.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # hypergraph blocks for English and French\n",
        "        self.en_hyper_blocks = nn.ModuleList([HyperGCNBlock(hidden_dim, hidden_dim, dropout) for _ in range(hyper_layers)])\n",
        "        self.fr_hyper_blocks = nn.ModuleList([HyperGCNBlock(hidden_dim, hidden_dim, dropout) for _ in range(hyper_layers)])\n",
        "\n",
        "        self.cross = CrossAttentionFusion(hidden_dim, n_heads=attn_heads, dropout=dropout)\n",
        "\n",
        "        # prediction head; we'll make start and end predictors over English sequence\n",
        "        self.pred_start = nn.Linear(hidden_dim, 1)\n",
        "        self.pred_end = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def encode_text(self, input_ids, attention_mask):\n",
        "        # returns sequence embeddings B x L x D\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=False, return_dict=True)\n",
        "        # use last hidden state\n",
        "        return out.last_hidden_state\n",
        "\n",
        "    def forward(self, en_input_ids, en_mask, fr_input_ids, fr_mask, H_en, H_fr):\n",
        "        # en_input_ids: B x L_en ; fr_input_ids: B x L_fr\n",
        "        B = en_input_ids.size(0)\n",
        "\n",
        "        en_seq = self.encode_text(en_input_ids, en_mask)  # B x L_en x D\n",
        "        fr_seq = self.encode_text(fr_input_ids, fr_mask)  # B x L_fr x D\n",
        "\n",
        "        # Flatten batch to apply hypergraph ops per sample (our HyperGraphConv expects N x F)\n",
        "        en_outs = []\n",
        "        fr_outs = []\n",
        "        for b in range(B):\n",
        "            Xe = en_seq[b]  # L_en x D\n",
        "            Xf = fr_seq[b]  # L_fr x D\n",
        "            He = H_en[b].to(Xe.device)  # L_en x E_en\n",
        "            Hf = H_fr[b].to(Xf.device)  # L_fr x E_fr\n",
        "\n",
        "            # apply hypergraph layers sequentially\n",
        "            for layer in self.en_hyper_blocks:\n",
        "                Xe = layer(Xe, He)\n",
        "            for layer in self.fr_hyper_blocks:\n",
        "                Xf = layer(Xf, Hf)\n",
        "\n",
        "            en_outs.append(Xe)\n",
        "            fr_outs.append(Xf)\n",
        "\n",
        "        # pad back to B x L x D (they are already that shape), stack\n",
        "        en_stack = torch.stack([e for e in en_outs], dim=0)\n",
        "        fr_stack = torch.stack([f for f in fr_outs], dim=0)\n",
        "\n",
        "        # cross-attention fusion\n",
        "        en_fused, fr_fused, pooled = self.cross(en_stack, fr_stack, en_mask==0 if en_mask is not None else None, fr_mask==0 if fr_mask is not None else None)\n",
        "\n",
        "        # predict on English fused tokens\n",
        "        start_logits = self.pred_start(en_fused).squeeze(-1)  # B x L_en\n",
        "        end_logits = self.pred_end(en_fused).squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits, pooled"
      ],
      "metadata": {
        "id": "Xc0Mm1uvWePC"
      },
      "id": "Xc0Mm1uvWePC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset & collate for bilingual inputs"
      ],
      "metadata": {
        "id": "6MhTctoEXe2X"
      },
      "id": "6MhTctoEXe2X"
    },
    {
      "cell_type": "code",
      "source": [
        "# For demo, we create a simple dataset that tokenizes question + context separately for English and French.\n",
        "\n",
        "# %%\n",
        "class SimpleBilingualQADataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer, max_len=128):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        q = ex['question_en']\n",
        "        c_en = ex['context_en']\n",
        "        c_fr = ex['context_fr']\n",
        "\n",
        "        enc_q = tokenizer(q, return_tensors='pt', add_special_tokens=True)\n",
        "        enc_en = tokenizer(c_en, return_tensors='pt', add_special_tokens=True)\n",
        "        enc_fr = tokenizer(c_fr, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "        return {\n",
        "            'id': ex['id'],\n",
        "            'q': q,\n",
        "            'context_en': c_en,\n",
        "            'context_fr': c_fr,\n",
        "            'enc_q': enc_q,\n",
        "            'enc_en': enc_en,\n",
        "            'enc_fr': enc_fr,\n",
        "            'answer_text': ex.get('answer_text', '')\n",
        "        }\n",
        "\n",
        "# collate that returns tensors and builds toy hypergraphs\n",
        "\n",
        "def collate_fn(batch):\n",
        "    B = len(batch)\n",
        "    # for simplicity ensure we use the same tokenization lengths as returned\n",
        "    en_ids = [b['enc_en']['input_ids'].squeeze(0) for b in batch]\n",
        "    en_mask = [b['enc_en']['attention_mask'].squeeze(0) for b in batch]\n",
        "    fr_ids = [b['enc_fr']['input_ids'].squeeze(0) for b in batch]\n",
        "    fr_mask = [b['enc_fr']['attention_mask'].squeeze(0) for b in batch]\n",
        "\n",
        "    # pad sequences\n",
        "    en_ids_padded = torch.nn.utils.rnn.pad_sequence(en_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    en_mask_padded = torch.nn.utils.rnn.pad_sequence(en_mask, batch_first=True, padding_value=0)\n",
        "    fr_ids_padded = torch.nn.utils.rnn.pad_sequence(fr_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    fr_mask_padded = torch.nn.utils.rnn.pad_sequence(fr_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    # build H_en and H_fr per sample\n",
        "    H_en_list = []\n",
        "    H_fr_list = []\n",
        "    for i, b in enumerate(batch):\n",
        "        L_en = en_ids[i].size(0)\n",
        "        L_fr = fr_ids[i].size(0)\n",
        "        # naive sentence boundaries: split on period tokens in the decoded text (very approximate)\n",
        "        txt_en = b['context_en']\n",
        "        sentences = [s.strip() for s in txt_en.split('.') if s.strip()]\n",
        "        s_bounds = []\n",
        "        cur = 1  # account for special token possibly at position 0\n",
        "        for s in sentences:\n",
        "            toks = tokenizer(s, add_special_tokens=False)\n",
        "            start = cur\n",
        "            end = cur + len(toks['input_ids']) - 1\n",
        "            s_bounds.append((start, min(L_en - 1, end)))\n",
        "            cur = end + 1\n",
        "        q_tokens = tokenizer(b['q'], add_special_tokens=False)\n",
        "        q_indices = list(range(1, 1 + len(q_tokens['input_ids']))) if len(q_tokens['input_ids'])>0 else []\n",
        "        H_en = build_simple_hypergraph(L_en, s_bounds, q_indices, window_size=4)\n",
        "\n",
        "        # French: similar\n",
        "        txt_fr = b['context_fr']\n",
        "        sentences = [s.strip() for s in txt_fr.split('.') if s.strip()]\n",
        "        s_bounds = []\n",
        "        cur = 1\n",
        "        for s in sentences:\n",
        "            toks = tokenizer(s, add_special_tokens=False)\n",
        "            start = cur\n",
        "            end = cur + len(toks['input_ids']) - 1\n",
        "            s_bounds.append((start, min(L_fr - 1, end)))\n",
        "            cur = end + 1\n",
        "        H_fr = build_simple_hypergraph(L_fr, s_bounds, [], window_size=4)\n",
        "\n",
        "        H_en_list.append(H_en)\n",
        "        H_fr_list.append(H_fr)\n",
        "\n",
        "    return {\n",
        "        'en_input_ids': en_ids_padded.to(device),\n",
        "        'en_mask': en_mask_padded.to(device),\n",
        "        'fr_input_ids': fr_ids_padded.to(device),\n",
        "        'fr_mask': fr_mask_padded.to(device),\n",
        "        'H_en': H_en_list,\n",
        "        'H_fr': H_fr_list,\n",
        "        'raw': batch\n",
        "    }\n"
      ],
      "metadata": {
        "id": "JBkXkriYWhcj"
      },
      "id": "JBkXkriYWhcj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instantiate model, optimizer, and a short training loop (demo)"
      ],
      "metadata": {
        "id": "1aajOuk3Xh7Q"
      },
      "id": "1aajOuk3Xh7Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "model = BilingualHypergraphQA(encoder=base_model, hidden_dim=base_model.config.hidden_size, hyper_layers=2, attn_heads=8, freeze_encoder=True).to(device)\n",
        "optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=3e-4)\n",
        "\n",
        "# small demo dataset\n",
        "train_dataset = SimpleBilingualQADataset([examples[0]], tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "3aWXvx7uWmgh"
      },
      "id": "3aWXvx7uWmgh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training step (very small demo — not meaningful metrics)"
      ],
      "metadata": {
        "id": "WRPNLan_XkZA"
      },
      "id": "WRPNLan_XkZA"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "model.train()\n",
        "for batch in train_loader:\n",
        "    en_ids = batch['en_input_ids']\n",
        "    en_mask = batch['en_mask']\n",
        "    fr_ids = batch['fr_input_ids']\n",
        "    fr_mask = batch['fr_mask']\n",
        "    H_en = batch['H_en']\n",
        "    H_fr = batch['H_fr']\n",
        "\n",
        "    start_logits, end_logits, pooled = model(en_ids, en_mask, fr_ids, fr_mask, H_en, H_fr)\n",
        "\n",
        "    # For demo, create toy target: find answer token positions by searching string in decoded text (approx)\n",
        "    raw = batch['raw'][0]\n",
        "    answer = raw['answer_text']\n",
        "    decoded_en = tokenizer.decode(en_ids[0], skip_special_tokens=True)\n",
        "    # naive index search\n",
        "    if answer in decoded_en:\n",
        "        char_idx = decoded_en.index(answer)\n",
        "        # approximate token-level mapping by tokenizing prefix\n",
        "        prefix = decoded_en[:char_idx]\n",
        "        prefix_toks = tokenizer(prefix, add_special_tokens=False)['input_ids']\n",
        "        start_pos = len(prefix_toks) + 1\n",
        "        answer_toks = tokenizer(answer, add_special_tokens=False)['input_ids']\n",
        "        end_pos = start_pos + len(answer_toks) - 1\n",
        "    else:\n",
        "        start_pos, end_pos = 1, 1\n",
        "\n",
        "    # build label tensors\n",
        "    B, L = start_logits.shape\n",
        "    start_labels = torch.zeros((B, L), device=device)\n",
        "    end_labels = torch.zeros((B, L), device=device)\n",
        "    start_labels[0, start_pos] = 1.0\n",
        "    end_labels[0, end_pos] = 1.0\n",
        "\n",
        "    loss_fct = nn.BCEWithLogitsLoss()\n",
        "    loss = loss_fct(start_logits, start_labels) + loss_fct(end_logits, end_labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    print('Demo loss:', loss.item())\n"
      ],
      "metadata": {
        "id": "wLmqc1fuWpkS"
      },
      "id": "wLmqc1fuWpkS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference helper to extract predicted span"
      ],
      "metadata": {
        "id": "mP5ruFMQXmy-"
      },
      "id": "mP5ruFMQXmy-"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_span(start_logits: torch.Tensor, end_logits: torch.Tensor, input_ids: torch.Tensor):\n",
        "    # greedy argmax\n",
        "    start_idx = torch.argmax(start_logits, dim=-1).item()\n",
        "    end_idx = torch.argmax(end_logits, dim=-1).item()\n",
        "    # clamp\n",
        "    start_idx = max(0, min(start_idx, input_ids.size(1)-1))\n",
        "    end_idx = max(start_idx, min(end_idx, input_ids.size(1)-1))\n",
        "    tokens = input_ids[0, start_idx:(end_idx+1)]\n",
        "    text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    return text, start_idx, end_idx"
      ],
      "metadata": {
        "id": "NP7set2fWvZy"
      },
      "id": "NP7set2fWvZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run the demo inference"
      ],
      "metadata": {
        "id": "JNmor7RCXrbW"
      },
      "id": "JNmor7RCXrbW"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        s_logits, e_logits, pooled = model(batch['en_input_ids'], batch['en_mask'], batch['fr_input_ids'], batch['fr_mask'], batch['H_en'], batch['H_fr'])\n",
        "        pred_text, s_idx, e_idx = predict_span(s_logits, e_logits, batch['en_input_ids'])\n",
        "        print('Predicted span:', pred_text)\n",
        "        print('Ground truth:', batch['raw'][0]['answer_text'])\n",
        "        em, f1 = compute_em_f1(pred_text, batch['raw'][0]['answer_text'])\n",
        "        print('EM:', em, 'F1:', f1)"
      ],
      "metadata": {
        "id": "jGHcDPJxWzGp"
      },
      "id": "jGHcDPJxWzGp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}