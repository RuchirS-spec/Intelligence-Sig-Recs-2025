{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üì¶ Step 0: Setup\n",
        "# ============================================================\n",
        "# Install specific package versions to ensure compatibility\n",
        "!pip install transformers sentence-transformers==2.2.2 huggingface_hub==0.14.1 networkx matplotlib torch -q\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests"
      ],
      "metadata": {
        "id": "X5BfI3DcbDeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üñºÔ∏è Step 1: Load or input images\n",
        "# ============================================================\n",
        "# You can replace these URLs with your own image files or upload images in Colab.\n",
        "urls = [\n",
        "    \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\",  # example image\n",
        "    \"https://images.unsplash.com/photo-1507525428034-b723cf961d3e\"                # beach image\n",
        "]\n",
        "images = [Image.open(requests.get(u, stream=True).raw).convert(\"RGB\") for u in urls]\n",
        "\n"
      ],
      "metadata": {
        "id": "y6x5t0_WbHc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üßæ Step 2: Generate captions using BLIP\n",
        "# ============================================================\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "captions = []\n",
        "for i, image in enumerate(images):\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    out = model.generate(**inputs, max_new_tokens=20)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "    captions.append(caption)\n",
        "    print(f\"üñºÔ∏è Image {i+1} Caption ‚Üí {caption}\")\n"
      ],
      "metadata": {
        "id": "NcW8mAYObECx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# üß© Step 3: Parse captions ‚Üí Scene Graphs (spaCy)\n",
        "# ============================================================\n",
        "!pip install spacy==3.8.1 -q\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def robust_scene_parse(caption):\n",
        "    doc = nlp(caption)\n",
        "    triples = []\n",
        "    for tok in doc:\n",
        "        # verb-based\n",
        "        if tok.pos_ == \"VERB\":\n",
        "            subs = [w.text for w in tok.lefts if w.dep_ in (\"nsubj\",\"nsubjpass\")]\n",
        "            objs = [w.text for w in tok.rights if w.dep_ in (\"dobj\",\"pobj\",\"attr\",\"obl\")]\n",
        "            for s in subs:\n",
        "                for o in objs:\n",
        "                    if s != o:\n",
        "                        triples.append((s, tok.lemma_, o))\n",
        "        # prepositions\n",
        "        if tok.dep_ == \"pobj\" and tok.head.pos_ == \"ADP\":\n",
        "            obj = tok.text\n",
        "            prep = tok.head.text\n",
        "            head = tok.head.head\n",
        "            subj = head.text\n",
        "            if head.pos_ == \"VERB\":\n",
        "                subs = [w.text for w in head.lefts if w.dep_ in (\"nsubj\",\"nsubjpass\")]\n",
        "                subj = subs[0] if subs else head.text\n",
        "            if head.dep_ == \"acl\" and head.head.pos_ == \"NOUN\":\n",
        "                subj = head.head.text\n",
        "            if subj and subj != obj:\n",
        "                triples.append((subj, prep, obj))\n",
        "        # adjectival clause (woman sitting)\n",
        "        if tok.dep_ == \"acl\" and tok.pos_ == \"VERB\":\n",
        "            subj = tok.head.text\n",
        "            objs = [w.text for w in tok.rights if w.dep_ in (\"pobj\",\"dobj\")]\n",
        "            for o in objs:\n",
        "                triples.append((subj, tok.lemma_, o))\n",
        "\n",
        "    clean = {(s.strip(), r.strip(), o.strip()) for s,r,o in triples if s.strip() and r.strip() and o.strip()}\n",
        "    return {\"relations\": [{\"subject\": {\"name\": s}, \"relation\": r, \"object\": {\"name\": o}} for s,r,o in clean]}\n"
      ],
      "metadata": {
        "id": "45gW_0-5bJ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üé® Step 4: Convert to NetworkX + visualize\n",
        "# ============================================================\n",
        "def scenegraph_to_networkx(graph_data):\n",
        "    G = nx.DiGraph()\n",
        "    for rel in graph_data[\"relations\"]:\n",
        "        s = rel[\"subject\"][\"name\"]\n",
        "        r = rel[\"relation\"]\n",
        "        o = rel[\"object\"][\"name\"]\n",
        "        G.add_node(s)\n",
        "        G.add_node(o)\n",
        "        G.add_edge(s, o, label=r)\n",
        "    return G\n",
        "\n",
        "def visualize_graph(G, title=\"Scene Graph\"):\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    nx.draw(G, pos, with_labels=True, node_color=\"skyblue\", node_size=1800,\n",
        "            edge_color=\"gray\", font_size=10, font_weight=\"bold\")\n",
        "    edge_labels = nx.get_edge_attributes(G, \"label\")\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Parse + visualize\n",
        "scene_graphs = [robust_scene_parse(c) for c in captions]\n",
        "nx_graphs = [scenegraph_to_networkx(g) for g in scene_graphs]\n",
        "for i,G in enumerate(nx_graphs):\n",
        "    visualize_graph(G, f\"Scene Graph {i+1}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ExxfipVUbMvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üîπ Step 5: Unsupervised Graph Embedding via Message Passing\n",
        "# ============================================================\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_emb(text):\n",
        "    return torch.tensor(embedder.encode(text, normalize_embeddings=True), dtype=torch.float)\n",
        "\n",
        "def get_graph_matrices(G):\n",
        "    nodes = list(G.nodes())\n",
        "    node_idx = {n:i for i,n in enumerate(nodes)}\n",
        "    A = nx.to_numpy_array(G, nodelist=nodes, weight=None)\n",
        "    A = A + np.eye(len(nodes))                      # self-loops\n",
        "    D_inv = np.diag(1 / A.sum(1))\n",
        "    A_norm = torch.tensor(D_inv @ A, dtype=torch.float)\n",
        "    X = torch.stack([get_emb(n) for n in nodes])\n",
        "    return A_norm, X\n",
        "\n",
        "def propagate(A_norm, X, K=2):\n",
        "    H = X.clone()\n",
        "    for _ in range(K):\n",
        "        H = A_norm @ H\n",
        "        H = F.normalize(H, p=2, dim=-1)\n",
        "    return H\n",
        "\n",
        "def graph_embedding(G, K=2):\n",
        "    A_norm, X = get_graph_matrices(G)\n",
        "    H = propagate(A_norm, X, K)\n",
        "    g_vec = H.mean(dim=0)\n",
        "    return F.normalize(g_vec, p=2, dim=0)\n",
        "\n",
        "# Compute graph vectors\n",
        "graph_vecs = [graph_embedding(G, K=2) for G in nx_graphs]\n",
        "\n",
        "# Cosine similarity\n",
        "sim = F.cosine_similarity(graph_vecs[0].unsqueeze(0), graph_vecs[1].unsqueeze(0)).item()\n",
        "print(f\"\\nüî¢ Semantic Graph Similarity (unsupervised): {sim:.4f}\")"
      ],
      "metadata": {
        "id": "tVkhoYCwbeRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
